{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - ENG TO FRE TRANSLATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I can unpickle the preprocessed sentences from the 2nd Jupyter Notebook for this project: \n",
    "#   (CAPSTONE PROJECT 2 - NLP - 2) Processing the data)\n",
    "import pickle\n",
    "\n",
    "#read the pickle file\n",
    "eng2_picklefile = open('preproc_english_sentences.pkl', 'rb')\n",
    "fre2_picklefile = open('preproc_french_sentences.pkl', 'rb')\n",
    "mfsl_picklefile = open('max_fre_seq_len.pkl', 'rb')\n",
    "evs_picklefile = open('eng_vocab_size.pkl', 'rb')\n",
    "fvs_picklefile = open('fre_vocab_size.pkl', 'rb')\n",
    "ft_picklefile = open('fre_tokenizer.pkl', 'rb')\n",
    "et_picklefile = open('eng_tokenizer.pkl', 'rb')\n",
    "fss_picklefile = open('fre_sentences_subset.pkl', 'rb')\n",
    "ess_picklefile = open('eng_sentences_subset.pkl', 'rb')\n",
    "\n",
    "#unpickle the objects\n",
    "preproc_english_sentences = pickle.load(eng2_picklefile)\n",
    "preproc_french_sentences = pickle.load(fre2_picklefile)\n",
    "max_french_sequence_length = pickle.load(mfsl_picklefile)\n",
    "english_vocab_size = pickle.load(evs_picklefile)\n",
    "french_vocab_size = pickle.load(fvs_picklefile)\n",
    "french_tokenizer = pickle.load(ft_picklefile)\n",
    "english_tokenizer = pickle.load(et_picklefile)\n",
    "french_sentences_subset = pickle.load(fss_picklefile)\n",
    "english_sentences_subset = pickle.load(ess_picklefile)\n",
    "\n",
    "#close files\n",
    "eng2_picklefile.close()\n",
    "fre2_picklefile.close()\n",
    "mfsl_picklefile.close()\n",
    "evs_picklefile.close()\n",
    "fvs_picklefile.close()\n",
    "ft_picklefile.close()\n",
    "et_picklefile.close()\n",
    "fss_picklefile.close()\n",
    "ess_picklefile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      "input_shape:\u001b[0m (6000, 138)\n",
      "\u001b[1m\u001b[32moutput_sequence_length:\u001b[0m 138\n",
      "\u001b[1m\u001b[32m\n",
      "english_vocab_size:\u001b[0m 8753\n",
      "\u001b[1m\u001b[32mfrench_vocab_size:\u001b[0m 12185\n"
     ]
    }
   ],
   "source": [
    "# Importing my_pad module from NLP helper functions\n",
    "from nlp_helper_functions import my_pad\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "tmp_x = my_pad.pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "print(colored('\\ninput_shape:','green',attrs=['bold']),tmp_x.shape)\n",
    "print(colored('output_sequence_length:','green',attrs=['bold']),max_french_sequence_length)\n",
    "print(colored('\\nenglish_vocab_size:','green',attrs=['bold']),english_vocab_size)\n",
    "print(colored('french_vocab_size:','green',attrs=['bold']),french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4800/4800 [==============================] - 388s 81ms/step - loss: 1.4697 - accuracy: 0.8101 - val_loss: 1.3881 - val_accuracy: 0.8151\n",
      "Epoch 2/10\n",
      "4800/4800 [==============================] - 378s 79ms/step - loss: 1.3028 - accuracy: 0.8169 - val_loss: 1.3835 - val_accuracy: 0.8172\n",
      "Epoch 3/10\n",
      "4800/4800 [==============================] - 375s 78ms/step - loss: 1.2326 - accuracy: 0.8204 - val_loss: 1.3851 - val_accuracy: 0.8164\n",
      "Epoch 4/10\n",
      "4800/4800 [==============================] - 365s 76ms/step - loss: 1.1687 - accuracy: 0.8232 - val_loss: 1.3994 - val_accuracy: 0.8149\n",
      "Epoch 5/10\n",
      "4800/4800 [==============================] - 371s 77ms/step - loss: 1.1113 - accuracy: 0.8257 - val_loss: 1.4120 - val_accuracy: 0.8137\n",
      "Epoch 6/10\n",
      "4800/4800 [==============================] - 369s 77ms/step - loss: 1.0600 - accuracy: 0.8281 - val_loss: 1.4340 - val_accuracy: 0.8137\n",
      "Epoch 7/10\n",
      "4800/4800 [==============================] - 375s 78ms/step - loss: 1.0138 - accuracy: 0.8302 - val_loss: 1.4445 - val_accuracy: 0.8124\n",
      "Epoch 8/10\n",
      "4800/4800 [==============================] - 371s 77ms/step - loss: 0.9735 - accuracy: 0.8324 - val_loss: 1.4666 - val_accuracy: 0.8125\n",
      "Epoch 9/10\n",
      "4800/4800 [==============================] - 371s 77ms/step - loss: 0.9372 - accuracy: 0.8347 - val_loss: 1.4805 - val_accuracy: 0.8134\n",
      "Epoch 10/10\n",
      "4800/4800 [==============================] - 374s 78ms/step - loss: 0.9043 - accuracy: 0.8367 - val_loss: 1.4970 - val_accuracy: 0.8128\n",
      "reprise de de session <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    \n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    embedding_dim = 512\n",
    "    input_seq = Input(input_shape[1:])\n",
    "    embedding = Embedding(english_vocab_size, embedding_dim)(input_seq)\n",
    "    \n",
    "    rnn = GRU(64, return_sequences=True)(embedding)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
    "    model = Model(input_seq, Activation('softmax')(logits))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                 optimizer=Adam(learning_rate),\n",
    "                 metrics=['accuracy'])    \n",
    "   \n",
    "    return model\n",
    "\n",
    "# Reshape the input\n",
    "tmp_x = my_pad.pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# Train the neural network\n",
    "embed_rnn_model = embed_model(tmp_x.shape, max_french_sequence_length, english_vocab_size, french_vocab_size)\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vous avez probablement appris par la presse et par la télévision que plusieurs attentats à la bombe et crimes ont été perpétrés au Sri Lanka.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_sentences_subset[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences_subset[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4800/4800 [==============================] - 376s 78ms/step - loss: 1.4708 - accuracy: 0.8100 - val_loss: 1.3879 - val_accuracy: 0.8145\n",
      "Epoch 2/10\n",
      "4800/4800 [==============================] - 376s 78ms/step - loss: 1.3023 - accuracy: 0.8171 - val_loss: 1.3816 - val_accuracy: 0.8167\n",
      "Epoch 3/10\n",
      "4800/4800 [==============================] - 375s 78ms/step - loss: 1.2305 - accuracy: 0.8204 - val_loss: 1.3846 - val_accuracy: 0.8161\n",
      "Epoch 4/10\n",
      "4800/4800 [==============================] - 375s 78ms/step - loss: 1.1661 - accuracy: 0.8231 - val_loss: 1.3958 - val_accuracy: 0.8140\n",
      "Epoch 5/10\n",
      "4800/4800 [==============================] - 376s 78ms/step - loss: 1.1074 - accuracy: 0.8257 - val_loss: 1.4164 - val_accuracy: 0.8149\n",
      "Epoch 6/10\n",
      "4800/4800 [==============================] - 377s 79ms/step - loss: 1.0552 - accuracy: 0.8278 - val_loss: 1.4353 - val_accuracy: 0.8132\n",
      "Epoch 7/10\n",
      "4800/4800 [==============================] - 375s 78ms/step - loss: 1.0083 - accuracy: 0.8304 - val_loss: 1.4469 - val_accuracy: 0.8132\n",
      "Epoch 8/10\n",
      "4800/4800 [==============================] - 377s 79ms/step - loss: 0.9673 - accuracy: 0.8327 - val_loss: 1.4720 - val_accuracy: 0.8115\n",
      "Epoch 9/10\n",
      "4800/4800 [==============================] - 370s 77ms/step - loss: 0.9297 - accuracy: 0.8350 - val_loss: 1.4910 - val_accuracy: 0.8100\n",
      "Epoch 10/10\n",
      "4800/4800 [==============================] - 375s 78ms/step - loss: 0.8978 - accuracy: 0.8371 - val_loss: 1.4983 - val_accuracy: 0.8102\n",
      "2 (2, 138)\n",
      "Sample 1:\n",
      "vous vous de un débat sur ce sujet sujet sujet ce <PAD> ce de de de de <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours\n",
      "\n",
      "Sample 2:\n",
      "reprise la la session <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "reprise de la session <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train neural network using model_final\n",
    "    x = pad_sequences(x, y.shape[1], padding='post')\n",
    "    model = embed_model(x.shape, y.shape[1], len(x_tk.word_index)+1, len(y_tk.word_index)+1)\n",
    "    model.fit(x, y, batch_size=1, epochs=10, validation_split=0.2)\n",
    "\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'You have requested a debate on this subject in the course of the next few days'\n",
    "    sentence = [x_tk.word_index[word.lower()] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    print(len(sentences), sentences.shape)\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Vous avez souhaité un débat à ce sujet dans les prochains jours')\n",
    "    print('\\nSample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "    \n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I was able to use more processing power with the use of a GPU and a better CPU setup by using a testing account I generated on Google Cloud Computing, the processing speed was not that great of an improvement over running this project on my own machine. The translation model performs well with short sentences, but it is having difficulty with longer sentences. This could be due to the fact that I had to subset the number of records (sentences) per language file to 6000 each. I would imagine that with more processing power and the ability to run this modeling on a substantially greater number of sentences, I would get better results than what I am currently seeing.  I may be able to tweak the modeling parameters to have it perform better as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
