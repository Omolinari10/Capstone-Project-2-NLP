{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - ENG TO FRE TRANSLATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I can unpickle the sentences from the 1st Jupyter Notebook for this project: \n",
    "#   (CAPSTONE PROJECT 2 - NLP - 1) Environment and Data Set Up\n",
    "import pickle\n",
    "\n",
    "#read the pickle file\n",
    "eng_picklefile = open('eng_sentences', 'rb')\n",
    "fre_picklefile = open('fre_sentences', 'rb')\n",
    "\n",
    "#unpickle the dataframes\n",
    "eng_sentences = pickle.load(eng_picklefile)\n",
    "fre_sentences = pickle.load(fre_picklefile)\n",
    "\n",
    "#close files\n",
    "eng_picklefile.close()\n",
    "fre_picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mEnglish Sentence 1 :  Resumption of the session\u001b[0m\n",
      "\u001b[1m\u001b[34mFrench Sentence 1  :  Reprise de la session\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mEnglish Sentence 2 :  I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\u001b[0m\n",
      "\u001b[1m\u001b[34mFrench Sentence 2  :  Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[32mEnglish Sentence 3 :  Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\u001b[0m\n",
      "\u001b[1m\u001b[34mFrench Sentence 3  :  Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "# Taking a look at a few examples of an English sentence and its French counterpart\n",
    "for sample_i in range(3):\n",
    "    print(colored('English Sentence {} :  {}'.format(sample_i+1, eng_sentences[sample_i]),'green', attrs=['bold']))\n",
    "    print(colored('French Sentence {}  :  {}\\n'.format(sample_i+1, fre_sentences[sample_i]), 'blue', attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Subsetting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Resumption of the session',\n",
       " 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subsetting the lists of sentences for ease of processing\n",
    "eng_sentences_subset = eng_sentences[:6000]\n",
    "fre_sentences_subset = fre_sentences[:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      "Word index dictionary:\u001b[0m {'is': 1, 'i': 2, 'cannot': 3, 'believe': 4, 'what': 5, 'you': 6, 'went': 7, 'through': 8, 'he': 9, 'who': 10, 'without': 11, 'sin': 12, 'cast': 13, 'the': 14, 'first': 15, 'stone': 16, 'this': 17, 'a': 18, 'short': 19, 'sentence': 20}\n",
      "\u001b[1m\u001b[32m\n",
      "Sequence 1 in x\u001b[0m\n",
      "  Input:  I cannot believe what you went through .\n",
      "  Output: [2, 3, 4, 5, 6, 7, 8]\n",
      "\u001b[1m\u001b[32m\n",
      "Sequence 2 in x\u001b[0m\n",
      "  Input:  He who is without sin cast the first stone .\n",
      "  Output: [9, 10, 1, 11, 12, 13, 14, 15, 16]\n",
      "\u001b[1m\u001b[32m\n",
      "Sequence 3 in x\u001b[0m\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [17, 1, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Example output\n",
    "\n",
    "# Importing my_tokenize module from NLP helper functions\n",
    "from nlp_helper_functions import my_tokenize\n",
    "\n",
    "text_sentences = [\n",
    "    'I cannot believe what you went through .',\n",
    "    'He who is without sin cast the first stone .',\n",
    "    'This is a short sentence .']\n",
    "\n",
    "text_tokenized, text_tokenizer = my_tokenize.tokenize(text_sentences)\n",
    "\n",
    "# Printing the word_index based on the above three sentences\n",
    "print(colored('\\nWord index dictionary:','green',attrs=['bold']),text_tokenizer.word_index)\n",
    "\n",
    "# Printing the input sentence followed by the output tokens corresponding to each word\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print(colored('\\nSequence {} in x'.format(sample_i + 1),'green',attrs=['bold']))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      "Sequence 1 in x\u001b[0m\n",
      "  Input:  [2 3 4 5 6 7 8]\n",
      "  Output: [2 3 4 5 6 7 8 0 0]\n",
      "\u001b[1m\u001b[32m\n",
      "Sequence 2 in x\u001b[0m\n",
      "  Input:  [ 9 10  1 11 12 13 14 15 16]\n",
      "  Output: [ 9 10  1 11 12 13 14 15 16]\n",
      "\u001b[1m\u001b[32m\n",
      "Sequence 3 in x\u001b[0m\n",
      "  Input:  [17  1 18 19 20]\n",
      "  Output: [17  1 18 19 20  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# Pad Tokenized output\n",
    "\n",
    "# Importing my_pad module from NLP helper functions\n",
    "from nlp_helper_functions import my_pad\n",
    "\n",
    "test_pad = my_pad.pad(text_tokenized)\n",
    "\n",
    "# Printing the input tokens followed by the output tokens with padding\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print(colored('\\nSequence {} in x'.format(sample_i + 1),'green',attrs=['bold']))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Generating a preprocess pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      "Data Preprocessed\n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[34mMax English sentence length:\u001b[0m 126\n",
      "\u001b[1m\u001b[34mMax French sentence length:\u001b[0m 138\n",
      "\u001b[1m\u001b[34m\n",
      "English vocabulary size:\u001b[0m 8753\n",
      "\u001b[1m\u001b[34mFrench vocabulary size:\u001b[0m 12185\n"
     ]
    }
   ],
   "source": [
    "# Importing my_preprocess module from NLP helper functions\n",
    "from nlp_helper_functions import my_preprocess\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    my_preprocess.preprocess(eng_sentences_subset, fre_sentences_subset)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
    "\n",
    "print(colored('\\nData Preprocessed\\n','green',attrs=['bold']))\n",
    "print(colored(\"Max English sentence length:\",'blue',attrs=['bold']), max_english_sequence_length)\n",
    "print(colored(\"Max French sentence length:\",'blue',attrs=['bold']), max_french_sequence_length)\n",
    "print(colored(\"\\nEnglish vocabulary size:\",'blue',attrs=['bold']), english_vocab_size)\n",
    "print(colored(\"French vocabulary size:\",'blue',attrs=['bold']), french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now pickle some of the objects to reuse later\n",
    "import pickle\n",
    "\n",
    "#create a file\n",
    "eng2_picklefile = open('preproc_english_sentences.pkl', 'wb')\n",
    "fre2_picklefile = open('preproc_french_sentences.pkl', 'wb')\n",
    "mfsl_picklefile = open('max_fre_seq_len.pkl', 'wb')\n",
    "evs_picklefile = open('eng_vocab_size.pkl', 'wb')\n",
    "fvs_picklefile = open('fre_vocab_size.pkl', 'wb')\n",
    "ft_picklefile = open('fre_tokenizer.pkl', 'wb')\n",
    "et_picklefile = open('eng_tokenizer.pkl', 'wb')\n",
    "fss_picklefile = open('fre_sentences_subset.pkl', 'wb')\n",
    "ess_picklefile = open('eng_sentences_subset.pkl', 'wb')\n",
    "\n",
    "#pickle the dataframe\n",
    "pickle.dump(preproc_english_sentences, eng2_picklefile)\n",
    "pickle.dump(preproc_french_sentences, fre2_picklefile)\n",
    "pickle.dump(max_french_sequence_length, mfsl_picklefile)\n",
    "pickle.dump(english_vocab_size, evs_picklefile)\n",
    "pickle.dump(french_vocab_size, fvs_picklefile)\n",
    "pickle.dump(french_tokenizer, ft_picklefile)\n",
    "pickle.dump(english_tokenizer, et_picklefile)\n",
    "pickle.dump(fre_sentences_subset, fss_picklefile)\n",
    "pickle.dump(eng_sentences_subset, ess_picklefile)\n",
    "\n",
    "#close file\n",
    "eng2_picklefile.close()\n",
    "fre2_picklefile.close()\n",
    "mfsl_picklefile.close()\n",
    "evs_picklefile.close()\n",
    "fvs_picklefile.close()\n",
    "ft_picklefile.close()\n",
    "et_picklefile.close()\n",
    "fss_picklefile.close()\n",
    "ess_picklefile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at an example sentence's words and corresponding tokens and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at an example of a sentence.\n",
    "eng_sentences_subset[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokens and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  34,   22,   13,  391,   43,    1, 1212,    4, 2217,    6,   32,\n",
       "         19,   40,    8,  266,    2, 2481, 5315,    4, 4011,    5, 2799,\n",
       "       3281,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the same sentence's tokens\n",
    "preproc_english_sentences[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
